{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2017/8/6 11:32\n",
    "# @Author  : play4fun\n",
    "# @File    : matchTemplate_credit_card_num1.py\n",
    "# @Software: PyCharm\n",
    "\n",
    "\"\"\"\n",
    "matchTemplate_credit_card_num1.py:\n",
    "\n",
    "http://www.pyimagesearch.com/2017/07/17/credit-card-ocr-with-opencv-and-python/\n",
    "\n",
    "python ocr_template_match.py --reference ocr_a_reference.png --image images/credit_card_04.png\n",
    "\n",
    "Credit Card Type: Visa\n",
    "Credit Card #: 4000123456789010\n",
    "\n",
    "\n",
    "检测图像中信用卡的位置。\n",
    "本地化四位数字，与信用卡上十六位数相关。\n",
    "应用OCR来识别信用卡上的十六位数字。\n",
    "识别信用卡类型（即Visa，万事达卡，美国运通等）。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# import the necessary packages\n",
    "from imutils import contours\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "# construct the argument parser and parse the arguments解析命令行参数\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-i\", \"--image\", required=True,\n",
    "#                 help=\"path to input image\")\n",
    "# ap.add_argument(\"-r\", \"--reference\", required=True,\n",
    "#                 help=\"path to reference OCR-A image\")\n",
    "# args = vars(ap.parse_args())\n",
    "args = {\"image\":\"img/card/card2.png\",\"reference\":\"img/card/reference.png\"}\n",
    "# define a dictionary that maps the first digit of a credit card\n",
    "# number to the credit card type定义信用卡类型\n",
    "FIRST_NUMBER = {\n",
    "    '0': 'None',\n",
    "    \"3\": \"American Express\",\n",
    "    \"4\": \"Visa\",\n",
    "    \"5\": \"MasterCard\",\n",
    "    \"6\": \"Discover Card\"\n",
    "}\n",
    "\n",
    "# load the reference OCR-A image from disk, convert it to grayscale,\n",
    "# and threshold it, such that the digits appear as *white* on a\n",
    "# *black* background\n",
    "# and invert it, such that the digits appear as *white* on a *black*\n",
    "ref = cv2.imread(args[\"reference\"])\n",
    "ref = cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY)\n",
    "ref = cv2.threshold(ref, 10, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "\n",
    "cv2.imshow('ref', ref)\n",
    "cv2.waitKey(1000)\n",
    "\n",
    "'''\n",
    "# find contours in the OCR-A image (i.e,. the outlines of the digits)\n",
    "# sort them from left to right, and initialize a dictionary to map\n",
    "# digit name to the ROI\n",
    "# refCnts = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)#有问题\n",
    "refCnts = cv2.findContours(ref.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# refCnts = refCnts[0] if imutils.is_cv2() else refCnts[1]\n",
    "refCnts = refCnts[1]\n",
    "print('len cnt:',len(refCnts))\n",
    "refCnts = contours.sort_contours(refCnts, method=\"left-to-right\")[0]#排列轮廓，没意义\n",
    "print('sort_contours len cnt:',len(refCnts))\n",
    "digits = {}\n",
    "\n",
    "# 循环浏览轮廓，提取ROI并将其与相应的数字相关联\n",
    "# loop over the OCR-A reference contours\n",
    "for (i, c) in enumerate(refCnts):\n",
    "    # compute the bounding box for the digit, extract it, and resize\n",
    "    # it to a fixed size\n",
    "    (x, y, w, h) = cv2.boundingRect(c)\n",
    "    roi = ref[y:y + h, x:x + w]\n",
    "    roi = cv2.resize(roi, (57, 88))\n",
    "    cv2.imshow('roi', roi)\n",
    "    cv2.waitKey(500)\n",
    "\n",
    "    # update the digits dictionary, mapping the digit name to the ROI\n",
    "    digits[i] = roi\n",
    "# 从参考图像中提取数字，并将其与相应的数字名称相关联\n",
    "print('digits:',digits.keys())\n",
    "'''\n",
    "\n",
    "# try1\n",
    "digits = {}\n",
    "rows, cols = ref.shape\n",
    "per = int(cols / 10)\n",
    "for x in range(10):\n",
    "    roi = ref[:, x * per:(x + 1) * per]\n",
    "    roi = cv2.resize(roi, (57, 88))\n",
    "    cv2.imshow('roi', roi)\n",
    "    cv2.waitKey(500)\n",
    "\n",
    "    # update the digits dictionary, mapping the digit name to the ROI\n",
    "    digits[x] = roi\n",
    "# 从参考图像中提取数字，并将其与相应的数字名称相关联\n",
    "print('digits:', digits.keys())\n",
    "\n",
    "# 初始化一对结构化的内核：\n",
    "# 您可以将内核看作是一个小矩阵，我们在图像上滑动以进行（卷积）操作，例如模糊，锐化，边缘检测或其他图像处理操作。\n",
    "# initialize a rectangular (wider than it is tall) and square\n",
    "# structuring kernel\n",
    "rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9, 3))\n",
    "sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "\n",
    "# 读取信用卡相片\n",
    "# load the input image, resize it, and convert it to grayscale\n",
    "image = cv2.imread(args[\"image\"])\n",
    "image = imutils.resize(image, width=300)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# apply a tophat (whitehat) morphological operator to find light\n",
    "# regions against a dark background (i.e., the credit card numbers)\n",
    "tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, rectKernel)\n",
    "\n",
    "# compute the Scharr gradient of the tophat image, then scale\n",
    "# the rest back into the range [0, 255]\n",
    "gradX = cv2.Sobel(tophat, ddepth=cv2.CV_32F, dx=1, dy=0,\n",
    "                  ksize=-1)\n",
    "gradX = np.absolute(gradX)\n",
    "(minVal, maxVal) = (np.min(gradX), np.max(gradX))\n",
    "gradX = (255 * ((gradX - minVal) / (maxVal - minVal)))\n",
    "gradX = gradX.astype(\"uint8\")\n",
    "\n",
    "# apply a closing operation using the rectangular kernel to help\n",
    "# cloes gaps in between credit card number digits, then apply\n",
    "# Otsu's thresholding method to binarize the image\n",
    "gradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, rectKernel)\n",
    "thresh = cv2.threshold(gradX, 0, 255,\n",
    "                       cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "# apply a second closing operation to the binary image, again\n",
    "# to help close gaps between credit card number regions\n",
    "thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel)\n",
    "\n",
    "# find contours in the thresholded image, then initialize the\n",
    "# list of digit locations找到轮廓并初始化数字分组位置列表。\n",
    "cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
    "                        cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts[0] if imutils.is_cv2() else cnts[1]\n",
    "locs = []\n",
    "\n",
    "# loop over the contours\n",
    "for (i, c) in enumerate(cnts):\n",
    "    # compute the bounding box of the contour, then use the\n",
    "    # bounding box coordinates to derive the aspect ratio\n",
    "    (x, y, w, h) = cv2.boundingRect(c)\n",
    "    ar = w / float(h)\n",
    "\n",
    "    # since credit cards used a fixed size fonts with 4 groups\n",
    "    # of 4 digits, we can prune potential contours based on the\n",
    "    # aspect ratio根据每个轮廓的宽高比进行过滤\n",
    "    if ar > 2.5 and ar < 4.0:\n",
    "        # contours can further be pruned on minimum/maximum width\n",
    "        # and height使用纵横比，我们分析每个轮廓的形状。如果 ar   在2.5到4.0之间（比它高），以及  40到55个像素之间的 w以及   10到20像素之间的h，我们将一个方便的元组的边界矩形参数附加到 locs\n",
    "        if (w > 40 and w < 55) and (h > 10 and h < 20):\n",
    "            # append the bounding box region of the digits group\n",
    "            # to our locations list\n",
    "            locs.append((x, y, w, h))\n",
    "\n",
    "# sort the digit locations from left-to-right, then initialize the\n",
    "# list of classified digits\n",
    "locs = sorted(locs, key=lambda x: x[0])\n",
    "output = []\n",
    "\n",
    "# loop over the 4 groupings of 4 digits\n",
    "for (i, (gX, gY, gW, gH)) in enumerate(locs):\n",
    "    # initialize the list of group digits\n",
    "    groupOutput = []\n",
    "\n",
    "    # extract the group ROI of 4 digits from the grayscale image,\n",
    "    # then apply thresholding to segment the digits from the\n",
    "    # background of the credit card\n",
    "    group = gray[gY - 5:gY + gH + 5, gX - 5:gX + gW + 5]\n",
    "    group = cv2.threshold(group, 0, 255,\n",
    "                          cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # detect the contours of each individual digit in the group,\n",
    "    # then sort the digit contours from left to right\n",
    "    digitCnts = cv2.findContours(group.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cv2.imshow('digitCnts', digitCnts[0])\n",
    "    cv2.waitKey(1000)\n",
    "    # digitCnts = digitCnts[0] if imutils.is_cv2() else digitCnts[1]\n",
    "    digitCnts = digitCnts[1]\n",
    "    # digitCnts = contours.sort_contours(digitCnts,method=\"left-to-right\")[0]\n",
    "\n",
    "    # loop over the digit contours\n",
    "    for c in digitCnts:\n",
    "        # compute the bounding box of the individual digit, extract\n",
    "        # the digit, and resize it to have the same fixed size as\n",
    "        # the reference OCR-A images\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "        roi = group[y:y + h, x:x + w]\n",
    "        roi = cv2.resize(roi, (57, 88))\n",
    "\n",
    "        # initialize a list of template matching scores\n",
    "        scores = []\n",
    "\n",
    "        # loop over the reference digit name and digit ROI\n",
    "        for (digit, digitROI) in digits.items():\n",
    "            # apply correlation-based template matching, take the\n",
    "            # score, and update the scores list\n",
    "            result = cv2.matchTemplate(roi, digitROI,\n",
    "                                       cv2.TM_CCOEFF)\n",
    "            (_, score, _, _) = cv2.minMaxLoc(result)\n",
    "            scores.append(score)\n",
    "\n",
    "        # the classification for the digit ROI will be the reference\n",
    "        # digit name with the *largest* template matching score\n",
    "        groupOutput.append(str(np.argmax(scores)))  # draw the digit classifications around the group\n",
    "        cv2.rectangle(image, (gX - 5, gY - 5),\n",
    "                      (gX + gW + 5, gY + gH + 5), (0, 0, 255), 2)\n",
    "        cv2.putText(image, \"\".join(groupOutput), (gX, gY - 15),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 255), 2)\n",
    "\n",
    "    # update the output digits list\n",
    "    output.extend(groupOutput)\n",
    "\n",
    "# display the output credit card information to the screen\n",
    "print(\"Credit Card Type: {}\".format(FIRST_NUMBER.get(output[0], 'None')))\n",
    "print(\"Credit Card #: {}\".format(\"\".join(output)))\n",
    "cv2.imshow(\"Image\", image)  # TODO 效果不是很好，需要改进\n",
    "cv2.waitKey(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2017/7/15 下午6:13\n",
    "# @Author  : play4fun\n",
    "# @File    : ocr.py.py\n",
    "# @Software: PyCharm\n",
    "\n",
    "\"\"\"\n",
    "ocr.py:\n",
    "\"\"\"\n",
    "\n",
    "# import the necessary packages\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-i\", \"--image\", required=True,\n",
    "#                 help=\"path to input image to be OCR'd\")\n",
    "# ap.add_argument(\"-p\", \"--preprocess\", type=str, default=\"thresh\",\n",
    "#                 help=\"type of preprocessing to be done\")\n",
    "# args = vars(ap.parse_args())\n",
    "args = {\"image\":\"img/card/tesserocr/\", \"preprocess\":\"thresh\"}\n",
    "# load the example image and convert it to grayscale\n",
    "image = cv2.imread(args[\"image\"])\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# check to see if we should apply thresholding to preprocess the\n",
    "# image\n",
    "if args[\"preprocess\"] == \"thresh\":\n",
    "    gray = cv2.threshold(gray, 0, 255,\n",
    "                         cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "# make a check to see if median blurring should be done to remove\n",
    "# noise\n",
    "elif args[\"preprocess\"] == \"blur\":\n",
    "    gray = cv2.medianBlur(gray, 3)\n",
    "\n",
    "# write the grayscale image to disk as a temporary file so we can\n",
    "# apply OCR to it\n",
    "filename = \"{}.png\".format(os.getpid())\n",
    "cv2.imwrite(filename, gray)\n",
    "\n",
    "# load the image as a PIL/Pillow image, apply OCR, and then delete\n",
    "# the temporary file\n",
    "text = pytesseract.image_to_string(Image.open(filename))\n",
    "os.remove(filename)\n",
    "print(text)\n",
    "\n",
    "# show the output images\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.imshow(\"Output\", gray)\n",
    "cv2.waitKey(10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装tesserocr方法：\n",
    "conda install -c simonflueckiger tesserocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2017/8/21 14:49\n",
    "# @Author  : play4fun\n",
    "# @File    : GetComponentImages-example.py.py\n",
    "# @Software: PyCharm\n",
    "\n",
    "\"\"\"\n",
    "GetComponentImages-example.py:\n",
    "\"\"\"\n",
    "\n",
    "from PIL import Image\n",
    "from tesserocr import PyTessBaseAPI, RIL\n",
    "\n",
    "with PyTessBaseAPI() as api:\n",
    "    # image = Image.open('/usr/src/tesseract/testing/phototest.tif')\n",
    "    image = Image.open('phototest.tif')  # 图片有问题\n",
    "    print(image.format, image.info, image.height, image.width)\n",
    "\n",
    "    api.SetImage(image)\n",
    "    boxes = api.GetComponentImages(RIL.TEXTLINE, True)\n",
    "    print('Found {} textline image components.'.format(len(boxes)))\n",
    "    for i, (im, box, _, _) in enumerate(boxes):\n",
    "        # im is a PIL image object\n",
    "        # box is a dict with x, y, w and h keys\n",
    "        api.SetRectangle(box['x'], box['y'], box['w'], box['h'])\n",
    "        ocrResult = api.GetUTF8Text()\n",
    "        conf = api.MeanTextConf()\n",
    "        print(u\"Box[{0}]: x={x}, y={y}, w={w}, h={h}, \"\n",
    "              \"confidence: {1}, text: {2}\").format(i, conf, ocrResult, **box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding:utf8-*-#\n",
    "\n",
    "__author__ = 'play4fun'\n",
    "\"\"\"\n",
    "create time:16/10/21 11:44\n",
    "\"\"\"\n",
    "\n",
    "from tesserocr import PyTessBaseAPI\n",
    "\n",
    "images = ['/Volumes/GF/Project/Python/Tesserocr/tesserocr/sample1.jpeg', '/Volumes/GF/Project/Python/Tesserocr/tesserocr/sample2.jpeg',\n",
    "          '/Volumes/GF/Project/Python/Tesserocr/tesserocr/sample3.jpeg']\n",
    "\n",
    "with PyTessBaseAPI() as api:\n",
    "    for img in images:\n",
    "        api.SetImageFile(img)\n",
    "        print('text:', api.GetUTF8Text())\n",
    "        print('-----')\n",
    "        print(api.AllWordConfidences())\n",
    "        print('-----')\n",
    "# api is automatically finalized when used in a with-statement (context manager).\n",
    "# otherwise api.End() should be explicitly called when it's no longer needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tesserocr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c7e86958c23f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \"\"\"\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtesserocr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tesserocr'"
     ]
    }
   ],
   "source": [
    "# -*-coding:utf8-*-#\n",
    "\n",
    "__author__ = 'play4fun'\n",
    "\"\"\"\n",
    "create time:16/10/21 11:47\n",
    "\"\"\"\n",
    "\n",
    "import tesserocr\n",
    "from PIL import Image\n",
    "\n",
    "print(tesserocr.tesseract_version())  # print tesseract-ocr version\n",
    "print(tesserocr.get_languages())  # prints tessdata path and list of available languages\n",
    "\n",
    "image = Image.open('sample.jpg')\n",
    "print(tesserocr.image_to_text(image))  # print ocr text from image\n",
    "# or\n",
    "print(tesserocr.file_to_text('sample.jpg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding:utf8-*-#\n",
    "\n",
    "__author__ = 'play4fun'\n",
    "\"\"\"\n",
    "create time:16/10/21 11:47\n",
    "\"\"\"\n",
    "# Orientation and script detection (OSD)\n",
    "\n",
    "from PIL import Image\n",
    "from tesserocr import PyTessBaseAPI, PSM\n",
    "\n",
    "with PyTessBaseAPI(psm=PSM.AUTO_OSD) as api:\n",
    "    # image = Image.open(\"/usr/src/tesseract/testing/eurotext.tif\")#No such file\n",
    "    # image = Image.open(\"eurotext.tif\")\n",
    "    image = Image.open('phototest.tif')\n",
    "    api.SetImage(image)\n",
    "    api.Recognize()\n",
    "\n",
    "    it = api.AnalyseLayout()\n",
    "    orientation, direction, order, deskew_angle = it.Orientation()\n",
    "    print(\"Orientation: {:d}\".format(orientation))\n",
    "    print(\"WritingDirection: {:d}\".format(direction))\n",
    "    print(\"TextlineOrder: {:d}\".format(order))\n",
    "    print(\"Deskew angle: {:.4f}\".format(deskew_angle))\n",
    "    #\n",
    "    ocrResult = api.GetUTF8Text()\n",
    "    print('result:\\n',ocrResult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "http://www.technicdynamic.com/2017/08/28/python-motion-detection-with-opencv-simple/\n",
    "\n",
    "'''\n",
    "\n",
    "import cv2  # importing Python OpenCV\n",
    "from datetime import datetime  # importing datetime for naming files w/ timestamp\n",
    "\n",
    "\n",
    "def diffImg(t0, t1, t2):  # Function to calculate difference between images.\n",
    "    d1 = cv2.absdiff(t2, t1)\n",
    "    d2 = cv2.absdiff(t1, t0)\n",
    "    return cv2.bitwise_and(d1, d2)\n",
    "\n",
    "\n",
    "threshold = 81500  # Threshold for triggering \"motion detection\"\n",
    "cam = cv2.VideoCapture(0)  # Lets initialize capture on webcam\n",
    "\n",
    "winName = \"Movement Indicator\"  # comment to hide window\n",
    "cv2.namedWindow(winName)  # comment to hide window\n",
    "\n",
    "# Read three images first:\n",
    "t_minus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\n",
    "t = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\n",
    "t_plus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\n",
    "# Lets use a time check so we only take 1 pic per sec\n",
    "timeCheck = datetime.now().strftime('%Ss')\n",
    "\n",
    "while True:\n",
    "    cv2.imshow(winName, cam.read()[1])  # comment to hide window\n",
    "    if cv2.countNonZero(diffImg(t_minus, t, t_plus)) > threshold and timeCheck != datetime.now().strftime('%Ss'):\n",
    "        dimg = cam.read()[1]\n",
    "        # cv2.imwrite(datetime.now().strftime('%Y%m%d_%Hh%Mm%Ss%f') + '.jpg', dimg)\n",
    "    timeCheck = datetime.now().strftime('%Ss')\n",
    "    # Read next image\n",
    "    t_minus = t\n",
    "    t = t_plus\n",
    "    t_plus = cv2.cvtColor(cam.read()[1], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    key = cv2.waitKey(10)\n",
    "    if key == ord('q'):\n",
    "        cv2.destroyWindow(winName)  # comment to hide window\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "https://www.learnopencv.com/object-tracking-using-opencv-cpp-python/\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set up tracker.\n",
    "    # Instead of MIL, you can also use\n",
    "\n",
    "    tracker_types = ['BOOSTING', 'MIL', 'KCF', 'TLD', 'MEDIANFLOW', 'GOTURN']\n",
    "    # tracker_type = tracker_types[2]\n",
    "    tracker_type = tracker_types[0]\n",
    "    tracker = cv2.Tracker_create(tracker_type)\n",
    "\n",
    "    # Read video\n",
    "    video = cv2.VideoCapture(\"videos/chaplin.mp4\")\n",
    "\n",
    "    # Exit if video not opened.\n",
    "    if not video.isOpened():\n",
    "        print(\"Could not open video\")\n",
    "        sys.exit()\n",
    "\n",
    "    # Read first frame.\n",
    "    ok, frame = video.read()\n",
    "    if not ok:\n",
    "        print('Cannot read video file')\n",
    "        sys.exit()\n",
    "\n",
    "    # Define an initial bounding box\n",
    "    bbox = (287, 23, 86, 320)\n",
    "\n",
    "    # Uncomment the line below to select a different bounding box\n",
    "    # bbox = cv2.selectROI(frame, False)\n",
    "\n",
    "    # Initialize tracker with first frame and bounding box\n",
    "    ok = tracker.init(frame, bbox)\n",
    "\n",
    "    while True:\n",
    "        # Read a new frame\n",
    "        ok, frame = video.read()\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        # Start timer\n",
    "        timer = cv2.getTickCount()\n",
    "\n",
    "        # Update tracker\n",
    "        ok, bbox = tracker.update(frame)\n",
    "\n",
    "        # Calculate Frames per second (FPS)\n",
    "        fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer);\n",
    "\n",
    "        # Draw bounding box\n",
    "        if ok:\n",
    "            # Tracking success\n",
    "            p1 = (int(bbox[0]), int(bbox[1]))\n",
    "            p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "            cv2.rectangle(frame, p1, p2, (255, 0, 0), 2, 1)\n",
    "        else:\n",
    "            # Tracking failure\n",
    "            cv2.putText(frame, \"Tracking failure detected\", (100, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n",
    "\n",
    "        # Display tracker type on frame\n",
    "        cv2.putText(frame, tracker_type + \" Tracker\", (100, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50, 170, 50), 2)\n",
    "\n",
    "        # Display FPS on frame\n",
    "        cv2.putText(frame, \"FPS : \" + str(int(fps)), (100, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50, 170, 50), 2)\n",
    "\n",
    "        # Display result\n",
    "        cv2.imshow(\"Tracking\", frame)\n",
    "\n",
    "        # Exit if ESC pressed\n",
    "        k = cv2.waitKey(1) & 0xff\n",
    "        if k == 27: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2017/7/27 11:55\n",
    "# @Author  : play4fun\n",
    "# @File    : 检测线条和形状-几何形状.py\n",
    "# @Software: PyCharm\n",
    "\n",
    "\"\"\"\n",
    "检测线条和形状-几何形状.py:\n",
    "\n",
    "https://stackoverflow.com/questions/31974843/detecting-lines-and-shapes-in-opencv-using-python\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class File(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def open(self, filename=None, mode='r'):\n",
    "        if filename is None:\n",
    "            filename = self.filename\n",
    "\n",
    "        return cv2.imread(filename), open(filename, mode)\n",
    "\n",
    "    def save(self, image=None, filename_override=None):\n",
    "        filename = \"output/\" + self.filename.split('/')[-1]\n",
    "\n",
    "        if filename_override:\n",
    "            filename = \"output/\" + filename_override\n",
    "\n",
    "        return cv2.imwrite(filename, image)\n",
    "\n",
    "\n",
    "class Image(object):\n",
    "    def __init__(self, image):\n",
    "        self.image = image\n",
    "\n",
    "    def grayscale(self):\n",
    "        return cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    def edges(self):\n",
    "        return cv2.Canny(self.image, 0, 255)\n",
    "\n",
    "    def lines(self):\n",
    "        lines = cv2.HoughLinesP(self.image, 1, np.pi / 2, 6, None, 50, 10)\n",
    "        for line in lines[0]:\n",
    "            pt1 = (line[0], line[1])\n",
    "            pt2 = (line[2], line[3])\n",
    "            cv2.line(self.image, pt1, pt2, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    File = File('images/children-draw.png')\n",
    "    Image = Image(File.open()[0])\n",
    "    Image.image = Image.grayscale()\n",
    "    Image.lines()\n",
    "    File.save(Image.image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
