{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected  4  face\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "'''\n",
    "以 Haar 特征分类器为基础的面部检测技术\n",
    "将面部检测扩展到眼部检测等。\n",
    "\n",
    "以 Haar 特征分类器为基础的对 检测技术是一种 常有效的对 检测 技术 2001 年 Paul_Viola 和 Michael_Jones 提出 。它是基于机器学习的    使用大 的正 样本图像 练得到一个 cascade_function 最后再用它 来做对 检测。\n",
    "现在我们来学习  检测。开始时 算法  大 的正样本图像   图 像 和 样本图像 不含  的图像 来 练分类器。我们  从其中提取特 征。下图中的 Haar 特征会 使用。它们就像我们的卷积核。每一个特征是一 个值  个值等于 色矩形中的像素值之后减去白色矩形中的像素值之和。\n",
    "\n",
    " 那么我们怎样从超过160000+ 个特征中 出最好的特征呢 ？\n",
    " 使用 Adaboost。\n",
    "\n",
    "OpenCV 自带了训练器和检测器。如果你想自己训练一个分类器来检测 汽车飞机等的\n",
    "可以使用 OpenCV 构建。\n",
    "Cascade Classifier Training :http://docs.opencv.org/2.4/doc/user_guide/ug_traincascade.html\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# 运行之前，检查cascade文件路径是否在你的电脑上\n",
    "face_cascade = cv2.CascadeClassifier('data/haarcascades/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('data/haarcascades/haarcascade_eye.xml')\n",
    "\n",
    "# img = cv2.imread('../data/sachin.jpg')\n",
    "# img = cv2.imread('../data/kongjie_hezhao.jpg')\n",
    "img = cv2.imread('img/airline-stewardess-bikini.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# cv2.imshow('gray', gray)\n",
    "\n",
    "\n",
    "# Detects objects of different sizes in the input image.\n",
    "# The detected objects are returned as a list of rectangles.\n",
    "# cv2.CascadeClassifier.detectMultiScale(image, scaleFactor, minNeighbors, flags, minSize, maxSize)\n",
    "# scaleFactor – Parameter specifying how much the image size is reduced at each image\n",
    "# scale.\n",
    "# minNeighbors – Parameter specifying how many neighbors each candidate rectangle should\n",
    "# have to retain it.\n",
    "# minSize – Minimum possible object size. Objects smaller than that are ignored.\n",
    "# maxSize – Maximum possible object size. Objects larger than that are ignored.\n",
    "# faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)#改进\n",
    "print(\"Detected \", len(faces), \" face\")\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    img = cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    roi_gray = gray[y:y + h, x:x + w]\n",
    "    roi_color = img[y:y + h, x:x + w]\n",
    "\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "    for (ex, ey, ew, eh) in eyes:\n",
    "        cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey(10000)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second using video.get(cv2.CAP_PROP_FPS) : 25.0\n",
      "共有 2499.0 帧\n",
      "高： 480.0 宽： 640.0\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 3 original boxes, 2 after suppression\n",
      "[INFO] 3 original boxes, 3 after suppression\n",
      "[INFO] 4 original boxes, 3 after suppression\n",
      "[INFO] 3 original boxes, 2 after suppression\n",
      "[INFO] 3 original boxes, 3 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 4 original boxes, 3 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 3 original boxes, 3 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 3 original boxes, 3 after suppression\n",
      "[INFO] 5 original boxes, 5 after suppression\n",
      "[INFO] 7 original boxes, 7 after suppression\n",
      "[INFO] 5 original boxes, 5 after suppression\n",
      "[INFO] 3 original boxes, 3 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 3 original boxes, 2 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 3 original boxes, 2 after suppression\n",
      "[INFO] 3 original boxes, 2 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 3 original boxes, 2 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 4 original boxes, 3 after suppression\n",
      "[INFO] 3 original boxes, 3 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 3 original boxes, 3 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 3 original boxes, 3 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 2 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 3 original boxes, 2 after suppression\n",
      "[INFO] 0 original boxes, 0 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 4 original boxes, 3 after suppression\n",
      "[INFO] 4 original boxes, 4 after suppression\n",
      "[INFO] 2 original boxes, 2 after suppression\n",
      "[INFO] 3 original boxes, 3 after suppression\n",
      "[INFO] 4 original boxes, 3 after suppression\n",
      "[INFO] 3 original boxes, 3 after suppression\n",
      "[INFO] 5 original boxes, 4 after suppression\n",
      "[INFO] 6 original boxes, 5 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n",
      "[INFO] 1 original boxes, 1 after suppression\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2017/7/23 下午4:14\n",
    "# @Author  : play4fun\n",
    "# @File    : Pedestrian_Detection_video.py\n",
    "# @Software: PyCharm\n",
    "\n",
    "\"\"\"\n",
    "Pedestrian_Detection_video.py:检测视频里的行人\n",
    "\n",
    "视频网站\n",
    "https://v.qq.com/x/page/t0501y6jtfi.html\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# import the necessary packages\n",
    "from __future__ import print_function\n",
    "from imutils.object_detection import non_max_suppression\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-i\", \"--images\", required=True, help=\"path to images directory\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "# initialize the HOG descriptor/person detector\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "#\n",
    "cap = cv2.VideoCapture('videos/礼让斑马线！齐齐哈尔城市文明的伤！.mp4')\n",
    "# cap = cv2.VideoCapture('../../data/TownCentreXVID.mp4')\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # 25.0\n",
    "print(\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n",
    "num_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "print('共有', num_frames, '帧')  # 共有 2499.0 帧\n",
    "\n",
    "frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "print('高：', frame_height, '宽：', frame_width)  # 高： 480.0 宽： 640.0\n",
    "# exit(0)\n",
    "\n",
    "\n",
    "# 跳过多少帧\n",
    "skips = 20\n",
    "\n",
    "# loop over the image paths\n",
    "# for imagePath in paths.list_images(args[\"images\"]):\n",
    "while cap.isOpened():\n",
    "\n",
    "    # load the image and resize it to (1) reduce detection time\n",
    "    # and (2) improve detection accuracy\n",
    "    # image = cv2.imread(imagePath)\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    image = frame\n",
    "\n",
    "    #\n",
    "    current = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "    if current % skips != 0:\n",
    "        continue\n",
    "\n",
    "    image = imutils.resize(image, width=min(400, image.shape[1]))\n",
    "    orig = image.copy()\n",
    "\n",
    "    # detect people in the image\n",
    "    (rects, weights) = hog.detectMultiScale(image, winStride=(4, 4),\n",
    "                                            padding=(8, 8), scale=1.05)\n",
    "\n",
    "    # draw the original bounding boxes\n",
    "    for (x, y, w, h) in rects:\n",
    "        cv2.rectangle(orig, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "    # apply non-maxima suppression to the bounding boxes using a\n",
    "    # fairly large overlap threshold to try to maintain overlapping\n",
    "    # boxes that are still people\n",
    "    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "    pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "\n",
    "    # draw the final bounding boxes\n",
    "    for (xA, yA, xB, yB) in pick:\n",
    "        cv2.rectangle(image, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "\n",
    "    # show some information on the number of bounding boxes\n",
    "    # filename = imagePath[imagePath.rfind(\"/\") + 1:]\n",
    "    # print(\"[INFO] {}: {} original boxes, {} after suppression\".format(\n",
    "    print(\"[INFO] {} original boxes, {} after suppression\".format(len(rects), len(pick)))\n",
    "\n",
    "    # show the output images\n",
    "    cv2.imshow(\"Before NMS\", orig)\n",
    "    cv2.imshow(\"After NMS\", image)\n",
    "    cv2.moveWindow(\"After NMS\", y=0, x=400)\n",
    "\n",
    "    cv2.waitKey(10000)\n",
    "#     if key == ord(\"q\"):\n",
    "#         break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建自己想检测的模块\n",
    "使用官方OpenCV应用程序：opencv_createsamples，opencv_annotation，opencv_traincascade和opencv_visualisation 实现\n",
    "\n",
    "\n",
    "原文:\n",
    "http://docs.opencv.org/3.2.0/dc/d88/tutorial_traincascade.html\n",
    "\n",
    "## 介绍\n",
    "使用弱分类器的升级级联包括两个主要阶段：培训和检测阶段。使用HAAR或LBP的模型的检测阶段在对象检测教程中进行了描述。本文档概述了培训您自己的弱分类器级联的功能。目前的指导将贯穿各个阶段：收集培训数据，准备培训数\n",
    "\n",
    "## 准备培训资料\n",
    "\n",
    "为了训练弱分类器的级联，我们需要一组积极的样本（包含要检测的实际对象）和一组负图像（包含您不想检测的所有内容）。必须手动准备一组负样本，而使用opencv_createsamples应用程序创建一组正样本。\n",
    "\n",
    "## 负样本\n",
    "\n",
    "负样本取自任意图像，不包含要检测的对象。这些生成样本的负图像应该列在一个特殊的负图像文件中，每个行包含一个图像路径（可以是绝对的或相对的）。请注意，负样本和样本图像也称为背景样本或背景图像，并在本文档中可互换使用。\n",
    "\n",
    "描述的图像可以具有不同的尺寸。然而，每个图像应该等于或大于所需的训练窗口大小（其对应于模型尺寸，大多数时间是对象的平均大小），因为这些图像用于将给定的负图像分割成多个图像具有此训练窗口大小的样本。\n",
    "\n",
    "这样一个负面描述文件的例子：\n",
    "\n",
    "目录结构：\n",
    "\n",
    "/ IMG\n",
    "  img1.jpg\n",
    "  img2.jpg\n",
    "bg.txt\n",
    "文件bg.txt：\n",
    "\n",
    "IMG / img1.jpg\n",
    "IMG / img2.jpg\n",
    "您将会使用一组负窗口样本来告诉机器学习步骤，在尝试找到感兴趣的对象时，在这种情况下，提升无法寻找的内容。\n",
    "\n",
    "## 阳性样品\n",
    "\n",
    "正样本由opencv_createsamples应用程序创建。它们被推动过程用于定义当试图找到您感兴趣的对象时实际寻找的模型。该应用程序支持生成正样本数据集的两种方式。\n",
    "\n",
    "您可以从单个正面对象图像生成一堆积极的。\n",
    "您可以自己提供所有的积极因素，只能使用该工具剪切出来，调整大小并将它们放在opencv所需的二进制格式中。\n",
    "虽然第一种方法对固定对象工作正常，如非常刚性的标志，但是对于较少刚性的对象，它往往会失败。在这种情况下，我们建议使用第二种方法。通过使用opencv_createsamples应用程序，网络上的许多教程甚至可以指示100个真实对象图像，可以导致比1000个人为生成的正面更好的模型。如果你决定采取第一种方法，请记住一些事情：\n",
    "\n",
    "请注意，在将其提交给上述应用程序之前，您需要使用多个单一的积极样本，因为它仅适用于透视变换。\n",
    "如果您想要一个健壮的模型，请采集涵盖范围广泛的可能在对象类中发生的变体的样本。例如，在面孔的情况下，您应该考虑不同的种族和年龄组，情绪和胡须风格。这也适用于使用第二种方法时。\n",
    "第一种方法采用单个对象图像，例如公司徽标，并通过随机旋转对象，改变图像强度以及将图像放置在任意背景上，从给定对象图像创建大量正样本。随机性的数量和范围可以通过opencv_createsamples应用程序的命令行参数来控制。\n",
    "\n",
    "命令行参数：\n",
    "\n",
    "* -vec <vec_file_name> ：包含训练样本的输出文件的名称。\n",
    "* -img <image_file_name> ：源对象图像（如公司徽标）。\n",
    "* -bg <background_file_name>：背景描述文件; 包含用作对象的随机变形版本的背景的图像列表。\n",
    "* -num <number_of_samples> ：生成的阳性样本数。\n",
    "* -bgcolor <background_color>：背景颜色（目前为灰度图像）背景颜色表示透明颜色。由于可能会出现压缩伪影，所以可以通过-bgthresh指定颜色容差的数量。具有bgcolor-bgthresh和bgcolor + bgthresh范围的所有像素都被解释为透明的。\n",
    "* -bgthresh <background_color_threshold>\n",
    "* -inv ：如果指定，颜色将被反转。\n",
    "* -randinv ：如果指定，颜色将随机反转。\n",
    "* -maxidev <max_intensity_deviation> ：前景样本中像素的最大强度偏差。\n",
    "* -maxxangle <max_x_rotation_angle> ：朝向x轴的最大旋转角度必须以弧度表示。\n",
    "* -maxyangle <max_y_rotation_angle> ：向y轴的最大旋转角度必须以弧度表示。\n",
    "* -maxzangle <max_z_rotation_angle> ：朝向z轴的最大旋转角度必须以弧度表示。\n",
    "* -show：有用的调试选项。如果指定，将显示每个样品。按Esc将继续样品创建过程，而不显示每个样品。\n",
    "* -w <sample_width> ：输出样本的宽度（以像素为单位）。\n",
    "* -h <sample_height> ：输出样本的高度（以像素为单位）。\n",
    "以这种方式运行opencv_createsamples时，使用以下过程创建一个示例对象实例：给定的源图像围绕所有三个轴随机旋转。所选择的角由限制-maxxangle，-maxyangle和-maxzangle。那么具有来自[bg_color-bg_color_threshold; bg_color + bg_c​​olor_threshold]范围被解释为透明。将白噪声添加到前景的强度。如果-inv指定了键，则前景像素强度被反转。如果-randinv指定了密钥，则算法随机选择是否应该对该样本应用反演。最后，所获得的图像被放置在从背景描述文件的任意的背景下，调整为所指定的所需的大小-w和-h和存储到VEC文件，-vec\n",
    "\n",
    "也可以从先前标记的图像的集合获得正样本，这是构建鲁棒对象模型时的期望方式。该集合由与背景描述文件类似的文本文件描述。该文件的每行对应一个图像。该行的第一个元素是文件名，后跟对象注释的数量，后跟数字描述边界矩形（x，y，width，height）的对象的坐标。\n",
    "\n",
    "描述文件的一个例子：\n",
    "\n",
    "目录结构：\n",
    "\n",
    "/ IMG\n",
    "  img1.jpg\n",
    "  img2.jpg\n",
    "info.dat\n",
    "文件info.dat：\n",
    "\n",
    "img / img1.jpg 1 140 100 45 45\n",
    "img / img2.jpg 2 100 200 50 50 50 30 25 25\n",
    "\n",
    "图像img1.jpg包含具有以下边界矩形坐标的单个对象实例：（140，100，45，45）。图像img2.jpg包含两个对象实例。\n",
    "\n",
    "为了从这样的收集中创建积极的样本，-info应该指定参数，而不是-img：\n",
    "\n",
    "* -info <collection_file_name> ：标记图像集合的描述文件。\n",
    "请注意，在这种情况下，这些参数-bg, -bgcolor, -bgthreshold, -inv, -randinv, -maxxangle, -maxyangle, -maxzangle被简单地忽略，不再使用。在这种情况下，样本创建的方案如下。通过从原始图像中切出提供的边界框，从给定图像中取出对象实例。然后它们被调整到目标样本大小（通过定义-w和-h），并存储在输出VEC-文件，由定义的-vec参数。无失真应用，所以只能影响参数是-w，-h，-show和-num。\n",
    "\n",
    "创建-info文件的手动过程也可以使用opencv_annotation工具完成。这是一个开源工具，用于在任何给定的图像中可视化地选择对象实例的感兴趣区域。以下小节将详细讨论如何使用此应用程序。\n",
    "\n",
    "## 额外的言论\n",
    "\n",
    "opencv_createsamples实用程序可用于检查存储在任何给定的正样本文件中的样本。为了做到这一点只-vec，-w并-h应指定的参数。\n",
    "vec文件的示例可在此处使用opencv/data/vec_files/trainingfaces_24-24.vec。它可以用于训练具有以下窗口大小的面部检测器：-w 24 -h 24。\n",
    "## 使用OpenCV的集成注释工具\n",
    "\n",
    "由于OpenCV 3.x社区一直在提供和维护用于生成-info文件的开放源代码注释工具。如果OpenCV应用程序在其中构建，该工具可以通过命令opencv_annotation访问。\n",
    "\n",
    "使用该工具非常简单。该工具接受几个必需的和一些可选的参数：\n",
    "\n",
    "* --annotations （必需）：注释txt文件的路径，您要存储注释的位置，然后传递给-info参数[example - /data/annotations.txt]\n",
    "* --images （必需）：包含与对象的图像的文件夹的路径[example - / data / testimages /]\n",
    "* --maxWindowHeight （可选）：如果输入图像的高度较大，则在此给定分辨率，请调整图像的大小，以便更容易的注释，使用--resizeFactor。\n",
    "* --resizeFactor （可选）：使用--maxWindowHeight参数时用于调整输入图像大小的因子。\n",
    "请注意，可选参数只能一起使用。下面可以看到可以使用的命令的例子\n",
    "\n",
    "opencv_annotation --annotations = / path / to / annotations / file.txt --images = / path / to / image / folder /\n",
    "此命令将启动一个包含将用于注释的第一个图像和鼠标光标的窗口。有关如何使用注释工具的视频，请点击此处。基本上有几个触发动作的按键。鼠标左键用于选择对象的第一个角，然后继续绘制，直到您正常，并且注册了第二个鼠标左键单击时停止。每次选择后，您都有以下选择：\n",
    "\n",
    "按c：确认注释，​​将注释转为绿色并确认其存储\n",
    "按d：从注释列表中删除最后一个注释（易于删除错误的注释）\n",
    "按n：继续下一张图片\n",
    "按ESC：这将退出注释软件\n",
    "最后你会得到一个可以传递给-infoopencv_createsamples参数的可用注释文件。\n",
    "\n",
    "## 级联训练\n",
    "\n",
    "下一步是基于预先准备的正数和负数据集，对弱分类器的升压级联进行实际训练。\n",
    "\n",
    "opencv_traincascade应用程序的命令行参数按目的分组：\n",
    "\n",
    "共同论点：\n",
    "* -data <cascade_dir_name>：训练有素的分类器应存放在哪里。预先手动创建该文件夹。\n",
    "* -vec <vec_file_name> ：带有正样本的vec文件（由opencv_createsamples实用程序创建）。\n",
    "* -bg <background_file_name>：背景描述文件。这是包含负样本图像的文件。\n",
    "* -numPos <number_of_positive_samples> ：每个分类阶段培训使用的阳性样本数。\n",
    "* -numNeg <number_of_negative_samples> ：每个分类阶段训练中使用的负样本数。\n",
    "* -numStages <number_of_stages> ：要训练的级联级数。\n",
    "* -precalcValBufSize <precalculated_vals_buffer_size_in_Mb>：预先计算的特征值的缓冲区大小（以Mb为单位）。您指定的快训练过程中更多的内存，但是请记住，-precalcValBufSize并-precalcIdxBufSize结合不应超过您可用的系统内存。\n",
    "* -precalcIdxBufSize <precalculated_idxs_buffer_size_in_Mb>：用于预先计算的特征索引（Mb）的缓冲区大小。您指定的快训练过程中更多的内存，但是请记住，-precalcValBufSize并-precalcIdxBufSize结合不应超过您可用的系统内存。\n",
    "* -baseFormatSave：这个说法在哈尔式的特征的情况下是实际的。如果指定，级联将以旧格式保存。这仅适用于向后兼容性原因，并允许用户坚持使用旧的已弃用的界面，至少使用较新界面对模型进行训练。\n",
    "* -numThreads <max_number_of_threads>：训练期间使用的最大线程数。请注意，根据您的机器和编译选项，实际使用的线程数可能会较低。默认情况下，如果您使用TBB支持构建OpenCV（此优化所需），则可以选择最大可用线程。\n",
    "* -acceptanceRatioBreakValue <break_value>：此参数用于确定您的模型应该如何精确地保持学习和何时停止。一个好的指导方针是训练不超过10e-5，以确保该模型不会超出您的训练数据。默认情况下，该值设置为-1以禁用此功能。\n",
    "级联参数：\n",
    "* -stageType <BOOST(default)>：阶段类型 目前仅支持升级分类器作为舞台类型。\n",
    "* -featureType<{HAAR(default), LBP}> ：特征类型：HAAR - 类似Haar的功能，LBP - 本地二进制模式。\n",
    "* -w <sampleWidth>：训练样本的宽度（以像素为单位）。必须具有与训练样本创建期间使用的完全相同的值（opencv_createsamples实用程序）。\n",
    "* -h <sampleHeight>：训练样本的高度（以像素为单位）。必须具有与训练样本创建期间使用的完全相同的值（opencv_createsamples实用程序）。\n",
    "提升的分类参数：\n",
    "* -bt <{DAB, RAB, LB, GAB(default)}> ：提升分类器的类型：DAB - Discrete AdaBoost，RAB - Real AdaBoost，LB - LogitBoost，GAB - Gentle AdaBoost。\n",
    "* -minHitRate <min_hit_rate>：分类器的每个阶段的最小期望命中率。总命中率可以估计为（min_hit_rate ^ number_of_stages），[164] §4.1。\n",
    "* -maxFalseAlarmRate <max_false_alarm_rate>：分类器每个阶段的最大期望误报率。总误报率可以估计为（max_false_alarm_rate ^ number_of_stages），[164] §4.1。\n",
    "* -weightTrimRate <weight_trim_rate>：指定是否使用修剪及其重量。一个体面的选择是0.95。\n",
    "* -maxDepth <max_depth_of_weak_tree>：弱树的最大深度。一个体面的选择是1，就是树桩的情况。\n",
    "* -maxWeakCount <max_weak_tree_count>：每个级联阶段弱树的最大数量。提升的分类器（阶段）将具有如此多的弱树（<= maxWeakCount），以实现给定的需要-maxFalseAlarmRate。\n",
    "哈尔式功能参数：\n",
    "* -mode <BASIC (default) | CORE | ALL>：选择训练中使用的Haar功能集的类型。BASIC仅使用直立功能，而ALL使用全套直立和45度旋转功能集。有关详细信息，请参阅[97]。\n",
    "局部二进制模式参数：局部二进制模式没有参数。\n",
    "opencv_traincascade应用程序完成工作后，训练好的级联将被保存在cascade.xml文件-data夹中。此文件夹中的其他文件是针对中断培训的情况而创建的，因此您可以在完成培训后将其删除。\n",
    "\n",
    "训练完毕，你可以测试你的级联分类器！\n",
    "\n",
    "## 可视化级联分类器\n",
    "\n",
    "有时可以将训练出的级联可视化，查看其选择的功能以及其阶段的复杂程度。为此OpenCV提供了一个opencv_visualisation应用程序。此应用程序具有以下命令：\n",
    "\n",
    "* --image （必需）：对象模型的引用图像的路径。这应该是一个注释，其尺寸[ -w，-h]传递给opencv_createsamples和opencv_traincascade应用程序。\n",
    "* --model （必需）：训练模型的路径，它应该在提供给-dataopencv_traincascade应用程序参数的文件夹中。\n",
    "* --data （可选）：如果提供了必须事先手动创建的数据文件夹，则会存储舞台输出和功能的视频。\n",
    "示例命令可以在下面看到\n",
    "\n",
    "opencv_visualisation --image = / data / object.png --model = / data / model.xml --data = / data / result /\n",
    "当前可视化工具的一些限制\n",
    "\n",
    "只处理使用opencv_traincascade工具训练的级联分类器模型，其中包含树桩作为决策树[默认设置]。\n",
    "提供的图像需要是具有原始模型尺寸的示例窗口，并传递给--image参数。\n",
    "HAAR / LBP面部模型的示例在Angelina Jolie的给定窗口上运行，该窗口具有与级联分类器文件相同的预处理 - > 24x24像素图像，灰度转换和直方图均衡：\n",
    "\n",
    "为每个阶段制作一个视频，每个功能可视化：\n",
    "\n",
    "![visualisation_video](visualisation_video.png \"visualisation_video\")\n",
    "\n",
    "每个阶段都存储为一个图像，以便将来验证这些功能：\n",
    "\n",
    "![visualisation_single_stage](visualisation_single_stage.png \"visualisation_single_stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] images\\p1.jpg: 3 original boxes, 3 after suppression\n",
      "[INFO] images\\p2.jpg: 3 original boxes, 3 after suppression\n",
      "[INFO] images\\p3.jpg: 4 original boxes, 4 after suppression\n",
      "[INFO] images\\p4.JPG: 6 original boxes, 5 after suppression\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2017/7/23 下午3:55\n",
    "# @Author  : play4fun\n",
    "# @File    : Pedestrian_Detection1.py\n",
    "# @Software: PyCharm\n",
    "\n",
    "\"\"\"\n",
    "Pedestrian_Detection1.py:\n",
    "\n",
    "网址\n",
    "http://www.pyimagesearch.com/2015/11/09/pedestrian-detection-opencv/\n",
    "\n",
    "运行\n",
    "python Pedestrian_Detection1.py --images images\n",
    "\"\"\"\n",
    "\n",
    "# import the necessary packages\n",
    "from __future__ import print_function\n",
    "from imutils.object_detection import non_max_suppression\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-i\", \"--images\", required=True, help=\"path to images directory\")\n",
    "# args = vars(ap.parse_args())\n",
    "args = {\"images\":\"images\"}\n",
    "\n",
    "# initialize the HOG descriptor/person detector\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# loop over the image paths\n",
    "for imagePath in paths.list_images(args[\"images\"]):\n",
    "    # load the image and resize it to (1) reduce detection time\n",
    "    # and (2) improve detection accuracy\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = imutils.resize(image, width=min(400, image.shape[1]))\n",
    "    orig = image.copy()\n",
    "\n",
    "    # detect people in the image\n",
    "    (rects, weights) = hog.detectMultiScale(image, winStride=(4, 4),\n",
    "                                            padding=(8, 8), scale=1.05)\n",
    "\n",
    "    # draw the original bounding boxes\n",
    "    for (x, y, w, h) in rects:\n",
    "        cv2.rectangle(orig, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "    # apply non-maxima suppression to the bounding boxes using a\n",
    "    # fairly large overlap threshold to try to maintain overlapping\n",
    "    # boxes that are still people\n",
    "    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "    pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "\n",
    "    # draw the final bounding boxes\n",
    "    for (xA, yA, xB, yB) in pick:\n",
    "        cv2.rectangle(image, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "\n",
    "    # show some information on the number of bounding boxes\n",
    "    filename = imagePath[imagePath.rfind(\"/\") + 1:]\n",
    "    print(\"[INFO] {}: {} original boxes, {} after suppression\".format(\n",
    "        filename, len(rects), len(pick)))\n",
    "\n",
    "    # show the output images\n",
    "    cv2.imshow(\"Before NMS\", orig)\n",
    "    cv2.moveWindow('Before NMS', x=0, y=0)\n",
    "    cv2.imshow(\"After NMS\", image)\n",
    "    cv2.moveWindow('After NMS', x=orig.shape[1], y=0)\n",
    "    cv2.waitKey(10000)\n",
    "#     if k==ord('q'):\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2017/8/7 19:34\n",
    "# @Author  : play4fun\n",
    "# @File    : MSER_create1.py\n",
    "# @Software: PyCharm\n",
    "\n",
    "\"\"\"\n",
    "MSER_create1.py:\n",
    "https://stackoverflow.com/questions/40443988/python-opencv-ocr-image-segmentation\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('WQbGH.jpg')\n",
    "img = img[5:-5, 5:-5, :]\n",
    "\n",
    "mser = cv2.MSER_create()\n",
    "\n",
    "# Resize the image so that MSER can work better\n",
    "img2 = cv2.resize(img, (img.shape[1] * 2, img.shape[0] * 2))#扩大\n",
    "\n",
    "gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "vis = img2.copy()\n",
    "\n",
    "regions = mser.detectRegions(gray)\n",
    "hulls = [cv2.convexHull(p.reshape(-1, 1, 2)) for p in regions[0]]\n",
    "cv2.polylines(vis, hulls, 1, (0, 255, 0))\n",
    "\n",
    "img3 = cv2.resize(vis, (img.shape[1], img.shape[0]))\n",
    "cv2.namedWindow('img', 0)\n",
    "cv2.imshow('img', img3)\n",
    "cv2.imwrite('mser-result.jpg', vis)\n",
    "cv2.waitKey(10000)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2018/2/8 16:09\n",
    "# @Author  : play4fun\n",
    "# @File    : Displaying a video feed with OpenCV and Tkinter.py\n",
    "# @Software: PyCharm\n",
    "\n",
    "\"\"\"\n",
    "Displaying a video feed with OpenCV and Tkinter.py:\n",
    "https://www.pyimagesearch.com/2016/05/30/displaying-a-video-feed-with-opencv-and-tkinter/\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# import the necessary packages\n",
    "from __future__ import print_function\n",
    "from photoboothapp import PhotoBoothApp\n",
    "from imutils.video import VideoStream\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-o\", \"--output\", required=True,\n",
    "                help=\"path to output directory to store snapshots\")\n",
    "ap.add_argument(\"-p\", \"--picamera\", type=int, default=-1,\n",
    "                help=\"whether or not the Raspberry Pi camera should be used\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# initialize the video stream and allow the camera sensor to warmup\n",
    "print(\"[INFO] warming up camera...\")\n",
    "vs = VideoStream(usePiCamera=args[\"picamera\"] > 0).start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "# start the app\n",
    "pba = PhotoBoothApp(vs, args[\"output\"])\n",
    "pba.root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2018/2/8 15:56\n",
    "# @Author  : play4fun\n",
    "# @File    : opencv-with-tkinter.py\n",
    "# @Software: PyCharm\n",
    "\n",
    "\"\"\"\n",
    "opencv-with-tkinter.py:\n",
    "https://www.pyimagesearch.com/2016/05/23/opencv-with-tkinter/\n",
    "\n",
    "不需要\n",
    "pip install image\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# import the necessary packages\n",
    "from tkinter import *\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "import tkinter.filedialog as tkFileDialog\n",
    "import cv2\n",
    "\n",
    "\n",
    "def select_image():\n",
    "    # grab a reference to the image panels\n",
    "    global panelA, panelB\n",
    "\n",
    "    # open a file chooser dialog and allow the user to select an input\n",
    "    # image\n",
    "    path = tkFileDialog.askopenfilename()\n",
    "\n",
    "    # ensure a file path was selected\n",
    "    if len(path) > 0:\n",
    "        # load the image from disk, convert it to grayscale, and detect\n",
    "        # edges in it\n",
    "        image = cv2.imread(path)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        edged = cv2.Canny(gray, 50, 100)\n",
    "\n",
    "        #  represents images in BGR order; however PIL represents\n",
    "        # images in RGB order, so we need to swap the channels\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # convert the images to PIL format...\n",
    "        image = Image.fromarray(image)\n",
    "        edged = Image.fromarray(edged)\n",
    "\n",
    "        # ...and then to ImageTk format\n",
    "        image = ImageTk.PhotoImage(image)\n",
    "        edged = ImageTk.PhotoImage(edged)\n",
    "\n",
    "        # if the panels are None, initialize them\n",
    "        if panelA is None or panelB is None:\n",
    "            # the first panel will store our original image\n",
    "            panelA = Label(image=image)\n",
    "            panelA.image = image\n",
    "            panelA.pack(side=\"left\", padx=10, pady=10)\n",
    "\n",
    "            # while the second panel will store the edge map\n",
    "            panelB = Label(image=edged)\n",
    "            panelB.image = edged\n",
    "            panelB.pack(side=\"right\", padx=10, pady=10)\n",
    "\n",
    "        # otherwise, update the image panels\n",
    "        else:\n",
    "            # update the pannels\n",
    "            panelA.configure(image=image)\n",
    "            panelB.configure(image=edged)\n",
    "            panelA.image = image\n",
    "            panelB.image = edged\n",
    "\n",
    "\n",
    "# initialize the window toolkit along with the two image panels\n",
    "root = Tk()\n",
    "panelA = None\n",
    "panelB = None\n",
    "\n",
    "# create a button, then when pressed, will trigger a file chooser\n",
    "# dialog and allow the user to select an input image; then add the\n",
    "# button the GUI\n",
    "btn = Button(root, text=\"Select an image\", command=select_image)\n",
    "btn.pack(side=\"bottom\", fill=\"both\", expand=\"yes\", padx=\"10\", pady=\"10\")\n",
    "\n",
    "# kick off the GUI\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2018/2/8 16:10\n",
    "# @Author  : play4fun\n",
    "# @File    : photoboothapp.py\n",
    "# @Software: PyCharm\n",
    "\n",
    "\"\"\"\n",
    "photoboothapp.py:\n",
    "\"\"\"\n",
    "\n",
    "# import the necessary packages\n",
    "from __future__ import print_function\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "import tkinter as tki\n",
    "import threading\n",
    "import datetime\n",
    "import imutils\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "class PhotoBoothApp:\n",
    "    def __init__(self, vs, outputPath):\n",
    "        # store the video stream object and output path, then initialize\n",
    "        # the most recently read frame, thread for reading frames, and\n",
    "        # the thread stop event\n",
    "        self.vs = vs\n",
    "        self.outputPath = outputPath\n",
    "        self.frame = None\n",
    "        self.thread = None\n",
    "        self.stopEvent = None\n",
    "\n",
    "        # initialize the root window and image panel\n",
    "        self.root = tki.Tk()\n",
    "        self.panel = None\n",
    "\n",
    "        # create a button, that when pressed, will take the current\n",
    "        # frame and save it to file\n",
    "        btn = tki.Button(self.root, text=\"Snapshot!\",\n",
    "                         command=self.takeSnapshot)\n",
    "        btn.pack(side=\"bottom\", fill=\"both\", expand=\"yes\", padx=10,\n",
    "                 pady=10)\n",
    "\n",
    "        # start a thread that constantly pools the video sensor for\n",
    "        # the most recently read frame\n",
    "        self.stopEvent = threading.Event()\n",
    "        self.thread = threading.Thread(target=self.videoLoop, args=())\n",
    "        self.thread.start()\n",
    "\n",
    "        # set a callback to handle when the window is closed\n",
    "        self.root.wm_title(\"PyImageSearch PhotoBooth\")\n",
    "        self.root.wm_protocol(\"WM_DELETE_WINDOW\", self.onClose)\n",
    "\n",
    "    def videoLoop(self):\n",
    "        # DISCLAIMER:\n",
    "        # I'm not a GUI developer, nor do I even pretend to be. This\n",
    "        # try/except statement is a pretty ugly hack to get around\n",
    "        # a RunTime error that  throws due to threading\n",
    "        try:\n",
    "            # keep looping over frames until we are instructed to stop\n",
    "            while not self.stopEvent.is_set():\n",
    "                # grab the frame from the video stream and resize it to\n",
    "                # have a maximum width of 300 pixels\n",
    "                self.frame = self.vs.read()\n",
    "                self.frame = imutils.resize(self.frame, width=300)\n",
    "\n",
    "                #  represents images in BGR order; however PIL\n",
    "                # represents images in RGB order, so we need to swap\n",
    "                # the channels, then convert to PIL and ImageTk format\n",
    "                image = cv2.cvtColor(self.frame, cv2.COLOR_BGR2RGB)\n",
    "                image = Image.fromarray(image)\n",
    "                image = ImageTk.PhotoImage(image)\n",
    "\n",
    "                # if the panel is not None, we need to initialize it\n",
    "                if self.panel is None:\n",
    "                    self.panel = tki.Label(image=image)\n",
    "                    self.panel.image = image\n",
    "                    self.panel.pack(side=\"left\", padx=10, pady=10)\n",
    "\n",
    "                # otherwise, simply update the panel\n",
    "                else:\n",
    "                    self.panel.configure(image=image)\n",
    "                    self.panel.image = image\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(\"[INFO] caught a RuntimeError\")\n",
    "\n",
    "    def takeSnapshot(self):\n",
    "        # grab the current timestamp and use it to construct the\n",
    "        # output path\n",
    "        ts = datetime.datetime.now()\n",
    "        filename = \"{}.jpg\".format(ts.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "        p = os.path.sep.join((self.outputPath, filename))\n",
    "\n",
    "        # save the file\n",
    "        cv2.imwrite(p, self.frame.copy())\n",
    "        print(\"[INFO] saved {}\".format(filename))\n",
    "\n",
    "    def onClose(self):\n",
    "        # set the stop event, cleanup the camera, and allow the rest of\n",
    "        # the quit process to continue\n",
    "        print(\"[INFO] closing...\")\n",
    "        self.stopEvent.set()\n",
    "        self.vs.stop()\n",
    "        self.root.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2018/2/8 16:30\n",
    "# @Author  : play4fun\n",
    "# @File    : kivy_cv1.py\n",
    "# @Software: PyCharm\n",
    "\n",
    "\"\"\"\n",
    "参考：https://github.com/kivy/kivy/blob/master/kivy/core/camera/camera_opencv.py\n",
    "\n",
    "kivy_cv1.py:\n",
    "https://gist.github.com/ExpandOcean/de261e66949009f44ad2\n",
    "\n",
    "pip install kivy\n",
    "\n",
    "问题：无显示\n",
    "\"\"\"\n",
    "\n",
    "# coding:utf-8\n",
    "from kivy.app import App\n",
    "from kivy.uix.image import Image\n",
    "from kivy.clock import Clock\n",
    "from kivy.graphics.texture import Texture\n",
    "import cv2\n",
    "\n",
    "\n",
    "class KivyCamera(Image):\n",
    "    def __init__(self, capture, fps, **kwargs):\n",
    "        super(KivyCamera, self).__init__(**kwargs)\n",
    "        self.capture = capture\n",
    "        Clock.schedule_interval(self.update, 1.0 / fps)\n",
    "\n",
    "    def update(self, dt):\n",
    "        ret, frame = self.capture.read()\n",
    "        if ret:\n",
    "            # convert it to texture\n",
    "            buf1 = cv2.flip(frame, 0)\n",
    "            buf = buf1.tostring()\n",
    "            image_texture = Texture.create(\n",
    "                size=(frame.shape[1], frame.shape[0]), colorfmt='bgr')\n",
    "            image_texture.blit_buffer(buf, colorfmt='bgr', bufferfmt='ubyte')\n",
    "            # display image from the texture\n",
    "            self.texture = image_texture\n",
    "\n",
    "\n",
    "class CamApp(App):\n",
    "    def build(self):\n",
    "        self.capture = cv2.VideoCapture(1)\n",
    "        self.my_camera = KivyCamera(capture=self.capture, fps=30)\n",
    "        return self.my_camera\n",
    "\n",
    "    def on_stop(self):\n",
    "        # without this, app will not exit even if the window is closed\n",
    "        self.capture.release()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    CamApp().run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
